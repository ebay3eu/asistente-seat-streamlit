import streamlit as st
import json
from openai import OpenAI
from pinecone import Pinecone

# --- Configuraci√≥n de la P√°gina y T√≠tulo ---
st.set_page_config(
    page_title="Asistente Virtual SEAT",
    page_icon="üöó",
    layout="centered"
)

st.title("üöó Asistente Virtual SEAT")

# --- Conexi√≥n a los servicios ---
try:
    PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
except FileNotFoundError:
    st.error("Error: No se encontraron las claves de API. Aseg√∫rate de configurar los 'Secrets' en Streamlit Cloud.")
    st.stop()

# --- Inicializaci√≥n de Clientes ---
@st.cache_resource
def get_clients():
    client_openai = OpenAI(api_key=OPENAI_API_KEY)
    pc = Pinecone(api_key=PINECONE_API_KEY)
    pinecone_index = pc.Index("asistente-seat")
    return client_openai, pinecone_index

client_openai, pinecone_index = get_clients()

# --- Funciones de L√≥gica ---
# A√ëADIDO: La funci√≥n ahora acepta el historial del chat
def extraer_criterios_de_busqueda(pregunta_usuario, historial_chat):
    # A√ëADIDO: Construimos un resumen de la conversaci√≥n para darle contexto
    conversacion_para_contexto = "\n".join([f"{msg['role']}: {msg['content']}" for msg in historial_chat])

    prompt = f"""
    Analiza la √∫ltima pregunta del usuario teniendo en cuenta el historial de la conversaci√≥n para darle contexto.
    HISTORIAL DE LA CONVERSACI√ìN:
    {conversacion_para_contexto}

    √öLTIMA PREGUNTA DEL USUARIO: "{pregunta_usuario}"

    Extrae los criterios de la √∫ltima pregunta en formato JSON. Si la √∫ltima pregunta es un seguimiento de la anterior (ej: 'y por menos de 30.000‚Ç¨?'), combina los criterios.
    El JSON debe tener dos claves:
    1. "precio_max": un entero con el precio m√°ximo si se menciona. Si no, 0.
    2. "descripcion": una cadena de texto que resuma TODAS las caracter√≠sticas que busca el usuario, combinando el historial si es necesario.

    Ejemplo con historial:
    - Historial: user: "busco un suv", assistant: "Tenemos Arona, Ateca y Tarraco"
    - √öltima pregunta: "y que sea h√≠brido"
    - JSON resultante: {{"precio_max": 0, "descripcion": "un suv que sea h√≠brido"}}

    Responde √∫nicamente con el objeto JSON.
    """
    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": prompt}],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        criterios = json.loads(response.choices[0].message.content)
        return criterios
    except Exception as e:
        st.error(f"Error al extraer criterios: {e}")
        return None

def busqueda_inteligente(criterios, top_k=5):
    # Esta funci√≥n no necesita cambios, ya opera con los criterios extra√≠dos
    filtro_metadata = {}
    if criterios.get("precio_max") and criterios["precio_max"] > 0:
        filtro_metadata["precio"] = {"$lte": criterios["precio_max"]}

    query_embedding = client_openai.embeddings.create(
        input=[criterios["descripcion"]], model="text-embedding-3-small"
    ).data[0].embedding
    
    res_busqueda = pinecone_index.query(vector=query_embedding, top_k=top_k, include_metadata=True, filter=filtro_metadata)

    if res_busqueda['matches']:
        contexto = [item['metadata']['texto'] for item in res_busqueda['matches']]
        return "\n\n---\n\n".join(contexto), criterios["descripcion"]

    st.info("La b√∫squeda inicial fue demasiado espec√≠fica. Intentando una b√∫squeda m√°s amplia...")
    pregunta_relajada = "dime todos los modelos de coche disponibles"
    query_embedding_relajado = client_openai.embeddings.create(input=[pregunta_relajada], model="text-embedding-3-small").data[0].embedding
    res_busqueda_relajada = pinecone_index.query(vector=query_embedding_relajado, top_k=top_k, include_metadata=True, filter=filtro_metadata)
    
    if res_busqueda_relajada['matches']:
        contexto = [item['metadata']['texto'] for item in res_busqueda_relajada['matches']]
        return "\n\n---\n\n".join(contexto), criterios["descripcion"]

    return None, None

# A√ëADIDO: La funci√≥n ahora acepta el historial del chat
def generar_respuesta_inteligente(pregunta_original, contexto, descripcion_buscada, historial_chat):
    prompt_sistema = f"""
    Eres "Asistente Virtual SEAT", un experto amable y servicial. Tu objetivo es ayudar al usuario a encontrar su coche ideal.
    La pregunta original del usuario fue: "{pregunta_original}"
    El usuario buscaba un coche con estas caracter√≠sticas: "{descripcion_buscada}"

    He realizado una b√∫squeda y he encontrado los siguientes modelos que podr√≠an encajar parcialmente:
    CONTEXTO ENCONTRADO:
    {contexto}

    Tu tarea es responder al usuario de forma inteligente y conversacional.
    Usa el contexto encontrado para formular tu respuesta. Si el contexto no coincide con alguna parte espec√≠fica de la descripci√≥n (ej: el usuario pidi√≥ "color verde"), expl√≠calo amablemente.
    """
    
    # A√ëADIDO: Construimos el payload de mensajes incluyendo el historial
    # Limitamos el historial a los √∫ltimos 4 mensajes para no exceder el l√≠mite de tokens
    mensajes_para_api = [{"role": "system", "content": prompt_sistema}] 
    mensajes_para_api.extend(historial_chat[-4:]) 
    mensajes_para_api.append({"role": "user", "content": pregunta_original})

    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o",
            messages=mensajes_para_api,
            temperature=0.5,
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"Error al generar la respuesta con OpenAI: {e}")
        return "Hubo un problema al generar la respuesta."

# --- Interfaz de la Aplicaci√≥n ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if not st.session_state.messages:
    with st.chat_message("assistant"):
        st.write("¬°Hola! Soy tu asistente virtual de SEAT. ¬øEn qu√© puedo ayudarte?")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Escribe tu pregunta aqu√≠..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            # A√ëADIDO: Pasamos el historial a las funciones de l√≥gica
            historial_relevante = st.session_state.messages[:-1] # Todo excepto la √∫ltima pregunta
            criterios = extraer_criterios_de_busqueda(prompt, historial_relevante)
            
            if criterios:
                contexto, descripcion = busqueda_inteligente(criterios)
                if contexto:
                    # A√ëADIDO: Pasamos el historial a la funci√≥n de respuesta
                    respuesta = generar_respuesta_inteligente(prompt, contexto, descripcion, historial_relevante)
                    st.write(respuesta)
                    st.session_state.messages.append({"role": "assistant", "content": respuesta})
                else:
                    st.write("Lo siento, no he encontrado ning√∫n modelo que cumpla esos criterios.")
                    st.session_state.messages.append({"role": "assistant", "content": "Lo siento, no he encontrado ning√∫n modelo que cumpla esos criterios."})
            else:
                st.write("No he podido entender tu petici√≥n. ¬øPuedes reformularla?")
                st.session_state.messages.append({"role": "assistant", "content": "No he podido entender tu petici√≥n. ¬øPuedes reformularla?"})