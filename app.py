import streamlit as st
import json
from openai import OpenAI
from pinecone import Pinecone

# --- Configuraci√≥n de la P√°gina y T√≠tulo ---
st.set_page_config(
    page_title="Asistente Virtual SEAT",
    page_icon="üöó",
    layout="centered"
)

st.title("üöó Asistente Virtual SEAT")
st.caption("Tu experto en la gama de veh√≠culos SEAT. Soy capaz de entender peticiones complejas y filtrar por precio.")

# --- Conexi√≥n a los servicios usando los "Secrets" de Streamlit ---
try:
    PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
except FileNotFoundError:
    st.error("Error: No se encontraron las claves de API. Aseg√∫rate de configurar los 'Secrets' en Streamlit Cloud.")
    st.stop()

# --- Inicializaci√≥n de Clientes ---
@st.cache_resource
def get_clients():
    client_openai = OpenAI(api_key=OPENAI_API_KEY)
    pc = Pinecone(api_key=PINECONE_API_KEY)
    pinecone_index = pc.Index("asistente-seat")
    return client_openai, pinecone_index

client_openai, pinecone_index = get_clients()

# --- MAGIA NUEVA (1): Funci√≥n para extraer criterios de la pregunta ---
def extraer_criterios_de_busqueda(pregunta_usuario):
    """
    Usa un LLM para convertir la pregunta en lenguaje natural a un objeto JSON
    con los criterios de b√∫squeda que podemos usar.
    """
    prompt = f"""
    Analiza la siguiente pregunta de un usuario y extr√°ela en un formato JSON.
    La pregunta es: "{pregunta_usuario}"

    El JSON debe tener dos claves:
    1. "precio_max": un entero con el precio m√°ximo si se menciona. Si no, 0.
    2. "descripcion": una cadena de texto que resuma todas las dem√°s caracter√≠sticas que busca el usuario (ej: 'coche grande con buen maletero', 'deportivo y potente', 'h√≠brido con techo panor√°mico').

    Ejemplos:
    - Pregunta: "un coche por menos de 30000 euros que sea bueno para viajar" -> JSON: {{"precio_max": 30000, "descripcion": "coche bueno para viajar"}}
    - Pregunta: "el m√°s potente y deportivo" -> JSON: {{"precio_max": 0, "descripcion": "el m√°s potente y deportivo"}}
    - Pregunta: "algo grande que pueda ser de color verde y con techo panoramico" -> JSON: {{"precio_max": 0, "descripcion": "algo grande de color verde y con techo panoramico"}}

    Responde √∫nicamente con el objeto JSON.
    """
    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": prompt}],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        criterios = json.loads(response.choices[0].message.content)
        return criterios
    except Exception as e:
        st.error(f"Error al extraer criterios: {e}")
        return None

# --- MAGIA NUEVA (2): Funci√≥n de b√∫squeda que se relaja si no encuentra ---
def busqueda_inteligente(criterios, top_k=5):
    """
    Realiza una b√∫squeda en Pinecone. Si la descripci√≥n es muy espec√≠fica y no
    devuelve resultados, la relaja a una b√∫squeda m√°s general.
    """
    filtro_metadata = {}
    if criterios.get("precio_max") and criterios["precio_max"] > 0:
        filtro_metadata["precio"] = {"$lte": criterios["precio_max"]}

    # --- Intento 1: B√∫squeda estricta con la descripci√≥n completa ---
    query_embedding = client_openai.embeddings.create(
        input=[criterios["descripcion"]], model="text-embedding-3-small"
    ).data[0].embedding
    
    res_busqueda = pinecone_index.query(
        vector=query_embedding,
        top_k=top_k,
        include_metadata=True,
        filter=filtro_metadata
    )

    # Si la b√∫squeda estricta funciona, devuelve los resultados
    if res_busqueda['matches']:
        contexto = [item['metadata']['texto'] for item in res_busqueda['matches']]
        # Devolvemos el contexto y la descripci√≥n original que s√≠ funcion√≥
        return "\n\n---\n\n".join(contexto), criterios["descripcion"]

    # --- Intento 2: B√∫squeda relajada (si la primera fall√≥) ---
    # Si no hubo resultados, es probable que la descripci√≥n fuera demasiado espec√≠fica
    # (ej: "color verde"). Ahora buscamos sin descripci√≥n, solo con el filtro de precio.
    # La pregunta que usaremos para el embedding ser√° m√°s gen√©rica.
    st.info("La b√∫squeda inicial fue demasiado espec√≠fica. Intentando una b√∫squeda m√°s amplia...")
    
    pregunta_relajada = "dime todos los modelos de coche disponibles"
    query_embedding_relajado = client_openai.embeddings.create(
        input=[pregunta_relajada], model="text-embedding-3-small"
    ).data[0].embedding

    res_busqueda_relajada = pinecone_index.query(
        vector=query_embedding_relajado,
        top_k=top_k,
        include_metadata=True,
        filter=filtro_metadata
    )
    
    if res_busqueda_relajada['matches']:
        contexto = [item['metadata']['texto'] for item in res_busqueda_relajada['matches']]
        # Devolvemos el contexto y la descripci√≥n original que fall√≥, para que el LLM sepa qu√© explicar
        return "\n\n---\n\n".join(contexto), criterios["descripcion"]

    return None, None


def generar_respuesta_inteligente(pregunta_original, contexto, descripcion_buscada):
    """
    Genera una respuesta que explica qu√© se encontr√≥ y qu√© no.
    """
    prompt_sistema = f"""
    Eres "Asistente Virtual SEAT", un experto amable y servicial. Tu objetivo es ayudar al usuario a encontrar su coche ideal.
    La pregunta original del usuario fue: "{pregunta_original}"
    El usuario buscaba un coche con estas caracter√≠sticas: "{descripcion_buscada}"

    He realizado una b√∫squeda y he encontrado los siguientes modelos que podr√≠an encajar parcialmente:
    CONTEXTO ENCONTRADO:
    {contexto}

    Tu tarea es responder al usuario de forma inteligente:
    1. Si el contexto parece coincidir bien con la descripci√≥n buscada, simplemente resume los resultados y pres√©ntalos.
    2. Si el contexto NO parece coincidir con alguna parte espec√≠fica de la descripci√≥n (ej: el usuario pidi√≥ "color verde" pero en el contexto no se menciona), expl√≠calo amablemente. Di qu√© es lo que NO encontraste, y presenta los resultados que S√ç encontraste como una alternativa.
    
    Ejemplo de respuesta inteligente:
    "He buscado un coche grande con techo panor√°mico y de color verde. Aunque en mi base de datos no tengo informaci√≥n espec√≠fica sobre los colores, s√≠ he encontrado estos modelos grandes que ofrecen el techo panor√°mico como extra opcional: [resume los resultados del contexto]."

    Responde de forma clara y √∫til.
    """
    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": prompt_sistema},
                {"role": "user", "content": pregunta_original}
            ],
            temperature=0.5,
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"Error al generar la respuesta con OpenAI: {e}")
        return "Hubo un problema al generar la respuesta."

# --- Interfaz de la Aplicaci√≥n ---
pregunta = st.text_input("Escribe aqu√≠ tu pregunta (ej: 'coche por menos de 30.000‚Ç¨', 'el m√°s potente y deportivo', 'h√≠brido con techo panor√°mico')", key="pregunta_usuario")